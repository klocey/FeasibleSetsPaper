The purpose of this study is to provide a conceptual road map for applying the feasible set approach to understanding variation in the shapes of ecological patterns, to extend the existing feasible set appraoch to the SSAD, and to introduce more efficient sampling algorithms for constructing feasible sets.


**The interpretation of pattern**

Haegeman and Loreau (2008?) focused on an SAD feasible set not based on N and S, i.e. all possible shapes of the SAD meeting a different set of constraints. They showed that ecological MaxEnt models that include a lot of information basically infer the most likely distribution from a feasible set that is so highly constrained that it only includes the observed distribution and distributions very much like it, and so, produce accurate but trivial predictions. Locey and White (2013) focused on the SAD feasible set determined by N and S because these variables are primary inputs to SAD models and are, mathematically, the two primary pieces of information needed to construct a finite and countable SAD feasible set.

**New feasible sets**


**New and Improved Algorithms**
Feasible sets can be immense and enumerating them can be untenable. For example, there are 8.9x10^14 unique shapes of the SAD for N = 1000 and S = 10, i.e. when the abundances of ten species sum to 1000. However, unique forms of the SAD gathered from small random samples can be used to characterize the properties of the feasible such as the distribution of statistical features (e.g. species evenness) within it. 
Locey & White (2013) took a conceptually simple and unbiased approach to sampling the feasible set of SAD shapes, an approach known as integer partitioning. This approach is based on the fact that there are a limited number of unordered ways that n integers can sum to a total q, and hence, a limited number of ways that the abundances of S unlabeled species can sum to a total abundance of N. These unordered configurations of integers are called integer partitions (Bóna 2006). For example, the feasible set for q = 6 and n = 3 is: {(4, 1, 1), (3, 2, 1), (2, 2, 2)}, where differently ordered configurations having the same integer values (e.g. (4, 1, 1), (1, 1, 4), (1, 4, 1)) represent the same integer partition, i.e. (4, 1, 1). In the same way, they represent the same frequency distribution (one 4 and two 1’s) and the same rank distribution (4, 1, 1). Consequently, each integer partition represents a unique shape of the SAD for a given N and S.	Sampling from the set of all possible SAD shapes means sampling from the set of all distinguishable SAD outcomes, i.e. the observable variation. Again, this is because SAD models and theories only predict the shape of the SAD and not which species has which abundance. In sampling from the feasible set of SAD shapes, we are effectively accounting for how variables such as N and S influence the shape of the SAD apart from how any particular shape might arise (e.g. [4, 1, 1] versus [1, 4, 1]) through random or non-random ecological or statistical sampling processes and mechanisms. In this way, the feasible set is the only way to address the following question: Regardless of what mechanisms or processes could produce a highly uneven SAD,  be they ecological or statistical, random or non-random, are nearly all possible SAD outcomes highly uneven and very similar?Though, use of integer partitioning to randomly sample the feasible set allows the feasible space to be characterized without generating all possible forms, the current sampling algorithms are computationally inefficient. All published partitioning algorithms sample the feasible set only with regards to the total, q. In this way, all partitions of q have the same probability of being drawn, regardless of the number of elements n. This means that randomly sampling the feasible set for a given q and n, requires generating partitions according to q and then rejecting those not having n elements, often resulting in impractically high rejection rates. For example, randomly generating one partition for q = 1000 and n = 10 requires drawing from a feasible set of nearly 2.4x1031 partitions, one of the roughly 8.9x1014 having 10 elements; a probability of nearly 3.7x10-17. 
Here, we present algorithms that greatly increase the efficiency of sampling the SAD and SSAD feasible sets. We explain each algorithm and develop Python and R based implementations of them. We test each algorithm for sampling bias and for speed against the method of Locey & White (2013). To reveal the practical gains of these new algorithms, we reanalyze the SAD datasets of Locey & White (2013), one of the largest and most diverse compilations of species abundance data, wherein it took more than 10000 compute hours to examine 60% of the available data (9562 of 15950 SADs). Our algorithms will allow us to examine a larger portion of the data in less time. Finally, we examine general characteristics of the SSAD feasible set, which provides a constraint-based explanation for previous connections made between the distribution of abundances among species (i.e. SAD) and the distribution of individuals of a species across the landscape (i.e. SSAD) and suggest a constraint-based explanation for Taylor’s Law, i.e., the increase in variance with increase in average abundance. Our work expands the feasible set approach to an additional ecological pattern, to values of q (e.g., total community abundance, total species abundance) and n (e.g. species richness, number of samples or quadrats) that were previously untenable. This work greatly advances the front of numerical constraint-based methods in ecology (Pueyo et al. 2007; Haegeman & Loreau 2008; Harte et al. 2008; Haegeman & Etienne 2010; Harte 2011; Locey & White 2013).##### METHODSHere, we develop efficient and unbiased integer partitioning algorithms to generate random samples of feasible sets for discrete ecological patterns such as the SAD and SSAD that are defined by a total q and composed of n elements, where both q and n are positive integers. Integer partitioning is a mature field of mathematics and algorithms for generating random partitions of q (e.g. Nijenhuis & Wilf 1978) are often implemented in mathematical environments (e.g. Sage, Maple, Mathematica). However, these approaches do not allow the random partitioning of q into exactly n elements. Here, we use two well-established theorems and a partitioning identity to develop a general method for generating a random partition of q having n elements. We then modify our method to allow elements having zero values. We begin by using two theorems to generate a random partition of q (see Appendix 1 for generating functions and recurrence relations).1)	For every integer q there are p(q) partitions having q or less elements. 2)	For every integer q there are pk(q) partitions having k or less as the first element.For example, there are 5 partitions of 4 and each has 4 or less as the first element; i.e. p(4) = 5, {(4), (3, 1), (2, 2), (2, 1, 1), (1, 1, 1, 1)}. Likewise, there are 4 partitions of 4 having 3 or less as the first element; p3(4) = 4, {(3, 1), (2, 2), (2, 1, 1), (1, 1, 1, 1)}. 	A random partition of q can be built, element by element, by iteratively applying these two theorems (Fig. 1).  Specifically, if we choose a random number x from 1 to p(q), we can say there are at least x partitions of q having some value k or less as the first element, i.e. x ≤  pk(q). Likewise, there must also be some value of k – 1, for which, there are less than x partitions of q having k – 1 or less as the first element, i.e. pk-1(q) < x. Putting these statements together, there must be a value k for which  pk-1(q) < x ≤ pk(q). In this way, we can find the value of the first element in one of the partitions by finding the value of k that satisfies pk-1(q) < x ≤  pk(q). Having found the value of the first element, we can decrease x by pk-1(q) and q by k, and then find the first element for this combination of smaller values. Repeating this process will sequentially build the partition until q = 0 and the sum of the partition is equal to the original value of q (Fig 1).	The above approach is similar to well-established methods of generating random partitions of q (e.g. Nijenhuis & Wilf 1978, Stojmenovic 2008). However, our goal is to generate random partitions of q having n elements. For this, we use a well-known integer partitioning identity to restrict the number of elements in a randomly chosen partition to n. Specifically, the number of partitions of q having n elements equals the number of partitions of q having n as the first element (Bóna 2006). This is because each partition of q having n elements corresponds to one unique partition of q having n as the first element (Bóna 2006). For example, consider the partition (3, 1), which can be illustrated with rows of dots, called a Ferrer’s diagram (Fig 1). In the Ferrer’s diagram for (3, 1) there are two rows, the largest having three dots. Flipping the diagram on its diagonal produces its conjugate (2, 1, 1), which has three rows, the largest row having two dots (Fig 1). So, the conjugate of (3, 1) is (2, 1, 1) and vice versa. Consequently, the first part of an integer partition determines the number of parts in its conjugate (Bóna 2006). This allows us to extend the problem of generating random partitions of q to random partitions of q having exactly n elements. That is, knowing the first element must be n so that its conjugate has n elements, we can decrease q by n and then generate a random partition for this decreased value of q having n or less as the first element. Once the partition is generated, we append n to the beginning of the partition and conjugate it to produce a random partition of the original q having exactly n elements (Fig 1).The approach outlined above begins with a randomly chosen number x between 1 and p(q). It then finds the value of k that satisfies the inequality pk-1(q) < x ≤  pk(q), which is the value of the first element of the partition. However, the question remains as to which value of k to start with and how to proceed to different values. Indeed, we could start with the smallest possible value of k (k = 1) and take a ‘bottom-up’ approach, or the largest possible value and take a ‘top-down’ approach, or even choose k at random and use a ‘divide-and-conquer’ method. These approaches differ only in how k is chosen and each builds the partition one element at a time. However, in the event that q is much larger than n, e.g. all trees in the 50 ha Barro-Colorado Island mapped forest plot where q ≈ 200,000 individuals and n ≈ 300 species, the three algorithms above will still be inefficient. This is because they would first generate a partition of, say, 200000 having 300 as the first element, but having as many as 199701 elements, and then conjugate it to produce a partition of 200000 having 300 elements. Clearly building the partition one element at a time would be inefficient in this case.An alternative approach is to build a partition using multiples of integers – the ‘multiplicity’ approach. Instead of finding the value of k corresponding to the first element, appending it, and moving on, we can instead ask how many times must k occur, i.e. the partitions of q having some multiple m of k. We can start with the smallest possible multiple (i.e. m = 1) and ask whether x is less than or equal to the number of partitions of q – k\*m  having less than k as the first part. This is because the set of partitions of q having a number of k’s equal to m actually contains the set of partitions of q – k\*m  having less than k as the first part (Appendix 1).  We can increase m by one until x ≤ pk(q – k*m), at which point we will have found the corresponding multiple of k. One drawback to this method is the time taken for the extra computation. Random partitions for q and n, with some parts having zero valuesThe above algorithms address distributions having positive values, such as the distribution of abundance among species (SAD). In contrast, some ecological patterns include zero values (e.g. absences). One example is the species spatial abundance distribution (SSAD), a frequency distribution that characterizes the number of quadrats, cells, or areas containing a given abundance of a species (Brown et al. 1995; Haegeman & Etienne 2010; Harte 2011). However, only small changes are needed to adapt the above approaches to cases allowing zero-valued parts. For example, let 10 unlabeled individuals occupy a landscape sectioned into quarters. The most aggregated distribution would be for all 10 to occupy the same quarter, [10, 0, 0, 0]. The least aggregated would be for 3 to occupy two quarters while 2 occupy the other two quarters, [3, 3, 2, 2]. In fact, the number of configurations for 10 unlabeled individuals distributed across 4 unlabeled sections equals the number of partitions of 10 having 4 or less parts, i.e. p4(10 + 4) = 23. Consequently, if n ≤  q, a random partition for q and n allowing for zero-valued parts, is simply a random partition for q having n or less parts, with zeros appended to ensure the final form of the partition has n parts.On the other hand if n > q a different approach is needed. To see this let 4 unlabeled individuals occupy a landscape sectioned into tenths. The most aggregated distribution would be for all 4 to occupy the same subsection, [4, 0, 0, 0, 0, 0, 0, 0, 0, 0] and the least aggregated configuration would be for 4 sections to have one individual and for 6 sections to have zero, i.e. [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]. In this way, the number of possible configurations for 4 unlabeled individuals distributed across 10 unlabeled sections is p(q). Consequently, if q < n, a random partition for q and n, allowing for zero-valued parts, is simply a random partition for q having q or less parts, with zeros appended to ensure the partition has n parts.Examining for bias and speedWe implemented the above algorithms in Python and R and made them freely available using a public Github repository (https://github.com/klocey/partitions). We also developed these algorithms into an R package (“rpartitions”) to be distributed on The Comprehensive R Archive Network (CRAN; http://cran.us.r-project.org/). We used kernel density curves to visually compare the results of the above algorithms to full feasible sets and random samples generated with the function implemented in the Sage mathematical environment (www.sagemath.org) that is based on the algorithm of Nijenhuis and Wilf (1978) and is the method used in Locey & White (2013). If our algorithms are unbiased, then their distributions will not differ in any systematic way from full feasible sets and random samples generated using the proven function implemented in Sage. Additionally, our Python and R packages include test files that conduct, among several other tests of our source code and partitioning functions, 2-sample t-tests and 2-sample Kolmogorov Smirnov tests on kernel density curves of the variance of logarithmically transformed abundances from random samples generated by the Sage software and random samples generated using the algorithms derived here. These tests are important to regularly run because additional code developments can corrupt source code. We compare the computational speed of our algorithms to that of the approach used in Locey & White (2013) (i.e., using Sage to generate random partitions for a given q and rejecting those not having n elements) across a range of values of q, n, and q-n ratios for which the latter method was likely to return random samples within reasonable time (one hour). Because Sage is coded in Python, our comparisons are made using the Python versions of our algorithms.Empirical Demonstration of the New Algorithms Locey & White (2013) analyzed the species abundance distributions (SADs) of 9562 sites of trees, bird, mammal, fungi, and prokaryote communities using a partitioning algorithm that sampled the feasible set according to total abundance N but not with respect to species richness S (i.e. the number of elements). Those data consisted, in part, of a subset of previously compiled datasets of site-specific species abundance data (see White et al. 2012), and included four continental-to-global scale surveys, including the Christmas Bird Count (129 sites) (National Audubon Society 2002),  North American Breeding Bird Survey (1,586 sites) (Sauer et al. 2011), Gentry’s Forest Transect Data Set (182 sites) (Phillips & Miller 2002), Forest Inventory Analysis (7,359 sites) (U.S. Department of Agriculture 2010), and one global-scale data compilation, the Mammal Community Database (42 sites) (Thibault et al. 2011). Locey & White (2013) also compiled abundance data at the species level from five microbial metagenome projects for a total of 264 SADs. Those data were obtained from the metagenomics server MG-RAST (Meyer et al. 2008). Metagenomic data were compiled into datasets representing aquatic prokaryotic communities (48 metagenomes) (Flores et al. 2011, www.catlin.com/en/Responsibility/CatlinArcticSurvey), terrestrial prokaryotic communities (92 metagenomes) (Chu et al. 2010; Fierer et al. 2012), and terrestrial fungal communities (124 metagenomes) (Amend et al. 2010). We refer the reader to Locey & White (2013) and White et al. (2012) for more thorough descriptions of those datasets.The inefficiency of the partitioning method used by Locey & White (2013) restricted their analyses to combinations of abundance N and species richness S, for which, there was a reasonable probability of generating a random integer partition of N with exactly S elements. This restriction allowed for only 60% of the available data to be examined despite more than 10000 compute hours worth of effort. We reanalyze those datasets using the algorithms developed here, which should allow for random samples of a greater number of SADs to be produced in less time.General characteristics of the SSAD feasible setBrown et al. (1995) revealed evidence that the general form of the SSAD, like that of the SAD, is characterized by a hollow-curve. In the sense of the SSAD, a hollow-curve implies that many areas are occupied by few or no individuals and that relatively few areas are occupied by many individuals. Both Brown et al. (1995) and Harte (2011) mechanistically couple the SSAD and SAD using arguments based on local niche differences and the constraint-based approach of entropy maximization, respectively. We generated random samples of the feasible set of the SSAD for ecologically realistic combinations of q and n, and examined their general features.